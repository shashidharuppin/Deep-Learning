{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing all the dependencies\nimport tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nimport keras\nfrom keras.models import Model\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Input, Reshape, Flatten, Lambda, Conv2DTranspose,UpSampling2D\nfrom keras.preprocessing import backend as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras import optimizers","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# download training and test data from mnist and reshape it\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\nX_train = X_train.astype('float32') / 255.\noutput_X_train = X_train.reshape(-1,28,28,1)\n\nX_test = X_test.astype('float32') / 255.\noutput_X_test = X_test.reshape(-1,28,28,1)\n\nprint(X_train.shape, X_test.shape)","execution_count":3,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 6s 0us/step\n(60000, 28, 28) (10000, 28, 28)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding some noise to data\n\ninput_x_train = output_X_train + 0.5 * np.random.normal(loc=0.0, scale=1.0, size=output_X_train.shape) \ninput_x_test = output_X_test + 0.5 * np.random.normal(loc=0.0, scale=1.0, size=output_X_test.shape)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (7, 7, 32)\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder = Model(input_img, decoded)\nm = 128\nn_epoch = 50\nadam = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\nautoencoder.compile(optimizer=adam, loss='binary_crossentropy',metrics=['accuracy'])\nhist = autoencoder.fit(input_x_train,output_X_train, epochs=n_epoch, batch_size=m, shuffle=True,validation_split=0.20)","execution_count":15,"outputs":[{"output_type":"stream","text":"Train on 48000 samples, validate on 12000 samples\nEpoch 1/50\n48000/48000 [==============================] - 3s 64us/step - loss: 0.2954 - accuracy: 0.8033 - val_loss: 0.1615 - val_accuracy: 0.8037\nEpoch 2/50\n48000/48000 [==============================] - 3s 54us/step - loss: 0.1442 - accuracy: 0.8062 - val_loss: 0.1340 - val_accuracy: 0.8087\nEpoch 3/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.1278 - accuracy: 0.8087 - val_loss: 0.1244 - val_accuracy: 0.8099\nEpoch 4/50\n48000/48000 [==============================] - 3s 54us/step - loss: 0.1206 - accuracy: 0.8098 - val_loss: 0.1186 - val_accuracy: 0.8108\nEpoch 5/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1160 - accuracy: 0.8106 - val_loss: 0.1149 - val_accuracy: 0.8114\nEpoch 6/50\n48000/48000 [==============================] - 3s 54us/step - loss: 0.1130 - accuracy: 0.8110 - val_loss: 0.1126 - val_accuracy: 0.8116\nEpoch 7/50\n48000/48000 [==============================] - 3s 57us/step - loss: 0.1108 - accuracy: 0.8114 - val_loss: 0.1105 - val_accuracy: 0.8124\nEpoch 8/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.1091 - accuracy: 0.8116 - val_loss: 0.1090 - val_accuracy: 0.8126\nEpoch 9/50\n48000/48000 [==============================] - 3s 62us/step - loss: 0.1077 - accuracy: 0.8118 - val_loss: 0.1078 - val_accuracy: 0.8126\nEpoch 10/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.1066 - accuracy: 0.8120 - val_loss: 0.1068 - val_accuracy: 0.8127\nEpoch 11/50\n48000/48000 [==============================] - 3s 66us/step - loss: 0.1056 - accuracy: 0.8121 - val_loss: 0.1058 - val_accuracy: 0.8130\nEpoch 12/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.1048 - accuracy: 0.8122 - val_loss: 0.1052 - val_accuracy: 0.8128\nEpoch 13/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1041 - accuracy: 0.8123 - val_loss: 0.1046 - val_accuracy: 0.8129\nEpoch 14/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1035 - accuracy: 0.8124 - val_loss: 0.1039 - val_accuracy: 0.8131\nEpoch 15/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1029 - accuracy: 0.8125 - val_loss: 0.1034 - val_accuracy: 0.8133\nEpoch 16/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1024 - accuracy: 0.8125 - val_loss: 0.1030 - val_accuracy: 0.8132\nEpoch 17/50\n48000/48000 [==============================] - 3s 54us/step - loss: 0.1020 - accuracy: 0.8126 - val_loss: 0.1025 - val_accuracy: 0.8134\nEpoch 18/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1016 - accuracy: 0.8126 - val_loss: 0.1022 - val_accuracy: 0.8135\nEpoch 19/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.1012 - accuracy: 0.8127 - val_loss: 0.1018 - val_accuracy: 0.8134\nEpoch 20/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1009 - accuracy: 0.8127 - val_loss: 0.1014 - val_accuracy: 0.8137\nEpoch 21/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1006 - accuracy: 0.8128 - val_loss: 0.1011 - val_accuracy: 0.8137\nEpoch 22/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.1003 - accuracy: 0.8128 - val_loss: 0.1009 - val_accuracy: 0.8137\nEpoch 23/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.1000 - accuracy: 0.8128 - val_loss: 0.1006 - val_accuracy: 0.8136\nEpoch 24/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.0998 - accuracy: 0.8129 - val_loss: 0.1004 - val_accuracy: 0.8138\nEpoch 25/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0995 - accuracy: 0.8129 - val_loss: 0.1002 - val_accuracy: 0.8137\nEpoch 26/50\n48000/48000 [==============================] - 3s 54us/step - loss: 0.0993 - accuracy: 0.8129 - val_loss: 0.1000 - val_accuracy: 0.8136\nEpoch 27/50\n48000/48000 [==============================] - 3s 54us/step - loss: 0.0991 - accuracy: 0.8129 - val_loss: 0.0999 - val_accuracy: 0.8139\nEpoch 28/50\n48000/48000 [==============================] - 3s 58us/step - loss: 0.0989 - accuracy: 0.8130 - val_loss: 0.0996 - val_accuracy: 0.8138\nEpoch 29/50\n48000/48000 [==============================] - 3s 54us/step - loss: 0.0987 - accuracy: 0.8130 - val_loss: 0.0994 - val_accuracy: 0.8138\nEpoch 30/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0986 - accuracy: 0.8130 - val_loss: 0.0993 - val_accuracy: 0.8140\nEpoch 31/50\n48000/48000 [==============================] - 3s 60us/step - loss: 0.0984 - accuracy: 0.8130 - val_loss: 0.0992 - val_accuracy: 0.8136\nEpoch 32/50\n48000/48000 [==============================] - 3s 59us/step - loss: 0.0982 - accuracy: 0.8131 - val_loss: 0.0990 - val_accuracy: 0.8138\nEpoch 33/50\n48000/48000 [==============================] - 3s 61us/step - loss: 0.0981 - accuracy: 0.8131 - val_loss: 0.0988 - val_accuracy: 0.8139\nEpoch 34/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.0979 - accuracy: 0.8131 - val_loss: 0.0987 - val_accuracy: 0.8138\nEpoch 35/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.0978 - accuracy: 0.8131 - val_loss: 0.0986 - val_accuracy: 0.8138\nEpoch 36/50\n48000/48000 [==============================] - 3s 57us/step - loss: 0.0977 - accuracy: 0.8131 - val_loss: 0.0984 - val_accuracy: 0.8140\nEpoch 37/50\n48000/48000 [==============================] - 3s 57us/step - loss: 0.0976 - accuracy: 0.8131 - val_loss: 0.0983 - val_accuracy: 0.8139\nEpoch 38/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0974 - accuracy: 0.8131 - val_loss: 0.0982 - val_accuracy: 0.8139\nEpoch 39/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0973 - accuracy: 0.8132 - val_loss: 0.0981 - val_accuracy: 0.8139\nEpoch 40/50\n48000/48000 [==============================] - 3s 58us/step - loss: 0.0972 - accuracy: 0.8132 - val_loss: 0.0980 - val_accuracy: 0.8140\nEpoch 41/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0971 - accuracy: 0.8132 - val_loss: 0.0979 - val_accuracy: 0.8140\nEpoch 42/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0970 - accuracy: 0.8132 - val_loss: 0.0978 - val_accuracy: 0.8140\nEpoch 43/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0969 - accuracy: 0.8132 - val_loss: 0.0977 - val_accuracy: 0.8140\nEpoch 44/50\n48000/48000 [==============================] - 3s 57us/step - loss: 0.0968 - accuracy: 0.8132 - val_loss: 0.0977 - val_accuracy: 0.8138\nEpoch 45/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.0967 - accuracy: 0.8132 - val_loss: 0.0976 - val_accuracy: 0.8142\nEpoch 46/50\n48000/48000 [==============================] - 3s 56us/step - loss: 0.0966 - accuracy: 0.8132 - val_loss: 0.0974 - val_accuracy: 0.8139\nEpoch 47/50\n48000/48000 [==============================] - 3s 57us/step - loss: 0.0965 - accuracy: 0.8133 - val_loss: 0.0974 - val_accuracy: 0.8140\nEpoch 48/50\n48000/48000 [==============================] - 3s 59us/step - loss: 0.0964 - accuracy: 0.8133 - val_loss: 0.0973 - val_accuracy: 0.8141\nEpoch 49/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0964 - accuracy: 0.8133 - val_loss: 0.0972 - val_accuracy: 0.8141\nEpoch 50/50\n48000/48000 [==============================] - 3s 55us/step - loss: 0.0963 - accuracy: 0.8133 - val_loss: 0.0972 - val_accuracy: 0.8139\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoded_imgs = autoencoder.predict(input_x_test,verbose=0)\n\n","execution_count":20,"outputs":[{"output_type":"stream","text":"Test loss: [[[2.85877577e-05]\n  [7.98769520e-07]\n  [1.85723920e-07]\n  [6.76978615e-08]\n  [3.39013795e-08]\n  [1.52269006e-08]\n  [1.10284848e-08]\n  [8.42408721e-09]\n  [1.07531584e-08]\n  [9.18886123e-09]\n  [6.56275034e-09]\n  [5.80113690e-09]\n  [4.08629885e-09]\n  [4.34241532e-09]\n  [5.70836711e-09]\n  [8.94673935e-09]\n  [1.16237429e-08]\n  [1.30292275e-08]\n  [1.59723612e-08]\n  [2.21995933e-08]\n  [4.11945784e-08]\n  [1.06447025e-07]\n  [1.89153610e-07]\n  [2.46219770e-07]\n  [3.06665470e-07]\n  [1.07945652e-06]\n  [3.29216959e-06]\n  [4.15523755e-05]]\n\n [[1.75772948e-05]\n  [2.71060117e-06]\n  [5.21307811e-07]\n  [1.19740406e-07]\n  [5.24305221e-08]\n  [3.48319915e-08]\n  [6.13277891e-08]\n  [9.44380503e-08]\n  [1.28863320e-07]\n  [1.16043971e-07]\n  [9.26729555e-08]\n  [8.45311021e-08]\n  [4.92068821e-08]\n  [5.18335277e-08]\n  [7.27237435e-08]\n  [1.20168124e-07]\n  [1.80889174e-07]\n  [3.04752916e-07]\n  [4.21388847e-07]\n  [6.71802525e-07]\n  [1.42936801e-06]\n  [1.95150483e-06]\n  [1.50803692e-06]\n  [9.15105772e-07]\n  [8.49696335e-07]\n  [2.02727460e-06]\n  [2.07799394e-06]\n  [2.97501128e-06]]\n\n [[2.80318964e-05]\n  [7.19448326e-06]\n  [2.34178060e-06]\n  [7.26551036e-07]\n  [4.62661205e-07]\n  [5.19966193e-07]\n  [1.32799562e-06]\n  [3.06368383e-06]\n  [3.46757361e-06]\n  [3.12627617e-06]\n  [2.07892572e-06]\n  [2.12297755e-06]\n  [1.23234770e-06]\n  [1.72430089e-06]\n  [2.75229695e-06]\n  [3.49753373e-06]\n  [5.64309858e-06]\n  [1.05953695e-05]\n  [1.37551415e-05]\n  [1.93814212e-05]\n  [3.58955149e-05]\n  [2.34325344e-05]\n  [8.90081901e-06]\n  [2.23138022e-06]\n  [1.36179926e-06]\n  [1.78334221e-06]\n  [9.85826546e-07]\n  [1.00270415e-06]]\n\n [[1.66925056e-05]\n  [7.92554238e-06]\n  [4.08686128e-06]\n  [1.99554984e-06]\n  [2.23008033e-06]\n  [4.30341152e-06]\n  [1.15913572e-05]\n  [2.61781315e-05]\n  [2.33861465e-05]\n  [1.39283802e-05]\n  [7.68807695e-06]\n  [7.54107487e-06]\n  [4.15672639e-06]\n  [5.88624334e-06]\n  [6.74383773e-06]\n  [6.16715761e-06]\n  [1.02637778e-05]\n  [1.53026522e-05]\n  [1.84542932e-05]\n  [1.90018272e-05]\n  [3.92297407e-05]\n  [2.06801033e-05]\n  [5.06570450e-06]\n  [1.30134038e-06]\n  [5.34443927e-07]\n  [9.39180893e-07]\n  [3.35062992e-07]\n  [1.91446830e-07]]\n\n [[9.28211466e-06]\n  [4.41668271e-06]\n  [3.38582959e-06]\n  [3.66842232e-06]\n  [8.36584150e-06]\n  [2.84142679e-05]\n  [1.33846755e-04]\n  [3.73760529e-04]\n  [2.77213083e-04]\n  [9.57358207e-05]\n  [3.83186052e-05]\n  [3.14753670e-05]\n  [1.09800640e-05]\n  [9.72627004e-06]\n  [6.24826771e-06]\n  [4.72743022e-06]\n  [8.73002045e-06]\n  [1.29731970e-05]\n  [1.77864367e-05]\n  [1.81364430e-05]\n  [4.28063322e-05]\n  [2.75019429e-05]\n  [8.36632807e-06]\n  [2.07962353e-06]\n  [6.62202581e-07]\n  [6.25709504e-07]\n  [1.30147768e-07]\n  [7.06903762e-08]]\n\n [[8.62882371e-06]\n  [5.84873578e-06]\n  [9.82750771e-06]\n  [1.34460324e-05]\n  [6.29400165e-05]\n  [4.64947341e-04]\n  [2.73924251e-03]\n  [1.21232523e-02]\n  [7.61386054e-03]\n  [2.27379915e-03]\n  [7.04933074e-04]\n  [3.52298346e-04]\n  [6.44742977e-05]\n  [2.64352475e-05]\n  [1.27499579e-05]\n  [1.00689467e-05]\n  [1.76839785e-05]\n  [2.32919519e-05]\n  [3.37028177e-05]\n  [2.98256018e-05]\n  [6.77340140e-05]\n  [5.93075492e-05]\n  [1.94860040e-05]\n  [4.88869864e-06]\n  [8.91946172e-07]\n  [9.05420450e-07]\n  [1.60051528e-07]\n  [4.96322734e-08]]\n\n [[1.35639657e-05]\n  [1.95626399e-05]\n  [7.38140661e-05]\n  [1.68082988e-04]\n  [1.42063573e-03]\n  [1.67178679e-02]\n  [7.78277665e-02]\n  [2.55300224e-01]\n  [2.39524826e-01]\n  [9.32499915e-02]\n  [3.34909223e-02]\n  [1.01609351e-02]\n  [1.47982582e-03]\n  [3.01722088e-04]\n  [1.88566279e-04]\n  [2.03555042e-04]\n  [3.18522332e-04]\n  [5.99986000e-04]\n  [9.98669304e-04]\n  [9.50289832e-04]\n  [1.13592879e-03]\n  [5.92743570e-04]\n  [9.73203714e-05]\n  [1.42463759e-05]\n  [1.19979575e-06]\n  [1.04340722e-06]\n  [1.71708265e-07]\n  [3.46239375e-08]]\n\n [[1.90514711e-05]\n  [5.04543059e-05]\n  [3.11335985e-04]\n  [8.82899039e-04]\n  [1.05153639e-02]\n  [8.75774026e-02]\n  [2.82284975e-01]\n  [7.06552446e-01]\n  [8.29198182e-01]\n  [7.48723745e-01]\n  [6.23777211e-01]\n  [4.14715916e-01]\n  [1.18836313e-01]\n  [3.21128145e-02]\n  [2.32769586e-02]\n  [2.35726014e-02]\n  [3.38648744e-02]\n  [7.62842596e-02]\n  [1.12593181e-01]\n  [9.03682932e-02]\n  [3.72353941e-02]\n  [6.72702491e-03]\n  [4.61472344e-04]\n  [3.16509104e-05]\n  [1.44933733e-06]\n  [1.20888080e-06]\n  [1.83767554e-07]\n  [4.20709760e-08]]\n\n [[2.38420635e-05]\n  [5.09864476e-05]\n  [3.17960745e-04]\n  [1.25822157e-03]\n  [1.60782859e-02]\n  [1.06729157e-01]\n  [3.24995518e-01]\n  [7.40962863e-01]\n  [8.87618482e-01]\n  [9.25594866e-01]\n  [9.45231676e-01]\n  [9.60735679e-01]\n  [9.18203175e-01]\n  [8.64779472e-01]\n  [8.52436364e-01]\n  [8.35870802e-01]\n  [8.17324996e-01]\n  [8.70094955e-01]\n  [8.73829722e-01]\n  [7.46237516e-01]\n  [3.46755117e-01]\n  [1.87430363e-02]\n  [6.47352892e-04]\n  [2.46582258e-05]\n  [8.99735994e-07]\n  [1.05384174e-06]\n  [2.48650508e-07]\n  [6.96612261e-08]]\n\n [[1.74706074e-05]\n  [3.81140089e-05]\n  [2.77530693e-04]\n  [1.61734049e-03]\n  [1.80111025e-02]\n  [7.02571422e-02]\n  [1.68589875e-01]\n  [3.56324166e-01]\n  [4.74368066e-01]\n  [5.64004660e-01]\n  [6.36079133e-01]\n  [8.12462270e-01]\n  [8.53125572e-01]\n  [9.04004931e-01]\n  [9.45385218e-01]\n  [9.45939958e-01]\n  [8.97637486e-01]\n  [8.72569442e-01]\n  [9.05959308e-01]\n  [9.30501223e-01]\n  [7.87559211e-01]\n  [7.63661712e-02]\n  [1.28129881e-03]\n  [3.78638615e-05]\n  [2.02069259e-06]\n  [2.11352744e-06]\n  [5.68580049e-07]\n  [1.39236704e-07]]\n\n [[1.00278357e-05]\n  [1.85116296e-05]\n  [1.54994050e-04]\n  [1.07737735e-03]\n  [8.23810790e-03]\n  [2.29044352e-02]\n  [4.69663665e-02]\n  [5.67571633e-02]\n  [4.06693369e-02]\n  [3.02451979e-02]\n  [2.73437425e-02]\n  [5.08364551e-02]\n  [7.23256618e-02]\n  [1.80546612e-01]\n  [3.08079153e-01]\n  [3.00699770e-01]\n  [2.23870337e-01]\n  [2.23449126e-01]\n  [5.62027037e-01]\n  [9.42572534e-01]\n  [9.26180363e-01]\n  [1.98542506e-01]\n  [2.85397354e-03]\n  [7.51240223e-05]\n  [6.40371536e-06]\n  [7.15920123e-06]\n  [1.67395513e-06]\n  [4.00525579e-07]]\n\n [[6.36447612e-06]\n  [1.38975583e-05]\n  [1.17999742e-04]\n  [8.31200217e-04]\n  [4.16820729e-03]\n  [9.65178944e-03]\n  [1.63017288e-02]\n  [8.29857308e-03]\n  [3.57419392e-03]\n  [1.61101378e-03]\n  [1.56735431e-03]\n  [2.11870763e-03]\n  [2.26505147e-03]\n  [5.03795827e-03]\n  [8.82578362e-03]\n  [1.36672640e-02]\n  [2.01737750e-02]\n  [5.00521436e-02]\n  [3.71668845e-01]\n  [9.39749479e-01]\n  [9.09507513e-01]\n  [2.36183956e-01]\n  [7.92071782e-03]\n  [2.63986323e-04]\n  [3.14524441e-05]\n  [2.08788024e-05]\n  [3.22068831e-06]\n  [8.79022764e-07]]\n\n [[4.71532258e-06]\n  [9.64571336e-06]\n  [8.36881445e-05]\n  [4.71666455e-04]\n  [1.93301868e-03]\n  [2.73305061e-03]\n  [3.35061434e-03]\n  [1.81789033e-03]\n  [6.93676178e-04]\n  [5.19166759e-04]\n  [5.89896052e-04]\n  [5.32271166e-04]\n  [3.51963157e-04]\n  [5.64717280e-04]\n  [1.07899820e-03]\n  [2.60346266e-03]\n  [8.07989109e-03]\n  [5.70439212e-02]\n  [5.36113143e-01]\n  [9.32742834e-01]\n  [8.70572329e-01]\n  [3.79591852e-01]\n  [4.52799238e-02]\n  [2.86538829e-03]\n  [3.64883570e-04]\n  [1.38867283e-04]\n  [1.33022531e-05]\n  [2.35894322e-06]]\n\n [[3.51419772e-06]\n  [5.82529492e-06]\n  [3.23682652e-05]\n  [1.13294751e-04]\n  [2.21624854e-04]\n  [2.77403451e-04]\n  [4.45688464e-04]\n  [2.18071087e-04]\n  [1.80970266e-04]\n  [2.16541346e-04]\n  [2.62920046e-04]\n  [2.51274701e-04]\n  [1.40517368e-04]\n  [1.96088396e-04]\n  [4.74597939e-04]\n  [2.43669446e-03]\n  [2.32421849e-02]\n  [3.72965544e-01]\n  [8.77526522e-01]\n  [9.24466789e-01]\n  [7.25750268e-01]\n  [2.47772396e-01]\n  [4.99951504e-02]\n  [7.41295423e-03]\n  [1.11982238e-03]\n  [1.62162512e-04]\n  [1.55465059e-05]\n  [3.86559032e-06]]\n\n [[2.46999230e-06]\n  [2.67895530e-06]\n  [1.11471027e-05]\n  [2.43626346e-05]\n  [2.53427006e-05]\n  [3.59248952e-05]\n  [7.02276084e-05]\n  [5.52298261e-05]\n  [1.44197154e-04]\n  [2.50895013e-04]\n  [3.13853932e-04]\n  [4.50256979e-04]\n  [2.29780388e-04]\n  [2.68474920e-04]\n  [6.35537261e-04]\n  [7.38735031e-03]\n  [1.89749777e-01]\n  [8.90665531e-01]\n  [9.55363214e-01]\n  [7.92016387e-01]\n  [2.77721137e-01]\n  [4.04345877e-02]\n  [7.15351989e-03]\n  [2.00879108e-03]\n  [7.01361510e-04]\n  [1.19241238e-04]\n  [1.37927618e-05]\n  [3.69094823e-06]]\n\n [[3.27622433e-06]\n  [3.64612811e-06]\n  [9.26729990e-06]\n  [1.35072860e-05]\n  [7.98560814e-06]\n  [8.68012739e-06]\n  [1.82076674e-05]\n  [2.55191117e-05]\n  [1.12288151e-04]\n  [2.31757338e-04]\n  [4.94498119e-04]\n  [7.08664826e-04]\n  [3.92878341e-04]\n  [5.64337359e-04]\n  [2.44026911e-03]\n  [6.60291314e-02]\n  [7.26726115e-01]\n  [9.66758609e-01]\n  [8.90797973e-01]\n  [1.56548098e-01]\n  [1.08339507e-02]\n  [1.59158965e-03]\n  [5.66215080e-04]\n  [2.44862109e-04]\n  [1.68350380e-04]\n  [6.23272063e-05]\n  [8.05519630e-06]\n  [2.01704597e-06]]\n\n [[4.08367077e-06]\n  [3.54000395e-06]\n  [5.14047451e-06]\n  [4.97573410e-06]\n  [2.04932962e-06]\n  [1.97374038e-06]\n  [5.46744604e-06]\n  [7.54598113e-06]\n  [3.10243049e-05]\n  [9.74927825e-05]\n  [2.93728546e-04]\n  [6.21886400e-04]\n  [6.47248293e-04]\n  [2.09870120e-03]\n  [2.70956419e-02]\n  [5.68552375e-01]\n  [9.42832172e-01]\n  [9.44414318e-01]\n  [4.65020537e-01]\n  [6.28621038e-03]\n  [3.95316572e-04]\n  [2.24415955e-04]\n  [1.95186672e-04]\n  [1.39471071e-04]\n  [1.01764439e-04]\n  [4.04119564e-05]\n  [5.99604482e-06]\n  [1.72385376e-06]]\n\n [[4.75098204e-06]\n  [3.16021487e-06]\n  [2.64023538e-06]\n  [2.72935540e-06]\n  [1.58508828e-06]\n  [2.02230240e-06]\n  [4.21512004e-06]\n  [5.12310589e-06]\n  [1.86817433e-05]\n  [6.45451655e-05]\n  [4.53840214e-04]\n  [1.18507328e-03]\n  [1.57707452e-03]\n  [1.16636092e-02]\n  [2.42838293e-01]\n  [9.15497184e-01]\n  [9.51732695e-01]\n  [6.89842284e-01]\n  [3.07331495e-02]\n  [3.89517518e-04]\n  [7.20076787e-05]\n  [6.55485346e-05]\n  [1.24539089e-04]\n  [9.00108134e-05]\n  [6.36880213e-05]\n  [3.37214005e-05]\n  [5.56849000e-06]\n  [2.31960007e-06]]\n\n [[5.78714389e-06]\n  [3.86106603e-06]\n  [1.95007783e-06]\n  [1.96681049e-06]\n  [1.49050607e-06]\n  [1.28581257e-06]\n  [2.29235729e-06]\n  [2.76251581e-06]\n  [8.15384010e-06]\n  [2.58982109e-05]\n  [2.06992248e-04]\n  [1.29707390e-03]\n  [6.13826374e-03]\n  [1.10155955e-01]\n  [8.29177976e-01]\n  [9.66809094e-01]\n  [8.36169422e-01]\n  [8.18035752e-02]\n  [1.24474964e-03]\n  [7.86720557e-05]\n  [4.55876288e-05]\n  [6.00907297e-05]\n  [1.23319536e-04]\n  [8.45911782e-05]\n  [6.24233362e-05]\n  [3.57939571e-05]\n  [4.84369775e-06]\n  [2.08217352e-06]]\n\n [[8.94590448e-06]\n  [4.23481106e-06]\n  [1.17358297e-06]\n  [1.01535034e-06]\n  [7.25636994e-07]\n  [7.55374629e-07]\n  [1.79082576e-06]\n  [1.65893380e-06]\n  [4.51626192e-06]\n  [1.09424327e-05]\n  [8.67650815e-05]\n  [1.84377003e-03]\n  [3.85468192e-02]\n  [7.05518186e-01]\n  [9.80161667e-01]\n  [9.52063620e-01]\n  [2.10556209e-01]\n  [3.94767476e-03]\n  [1.65078032e-04]\n  [3.12447883e-05]\n  [4.94077540e-05]\n  [6.53861571e-05]\n  [7.61657866e-05]\n  [4.93431071e-05]\n  [4.25904036e-05]\n  [2.50949179e-05]\n  [3.92634456e-06]\n  [1.90406718e-06]]\n\n [[1.79171348e-05]\n  [9.17481520e-06]\n  [1.77125878e-06]\n  [9.20708146e-07]\n  [4.75488918e-07]\n  [5.85721978e-07]\n  [1.41826615e-06]\n  [1.10461247e-06]\n  [3.76530693e-06]\n  [1.13421065e-05]\n  [9.07293506e-05]\n  [3.62708536e-03]\n  [2.35153839e-01]\n  [9.65168238e-01]\n  [9.85370398e-01]\n  [5.75846314e-01]\n  [1.16728041e-02]\n  [8.36262421e-04]\n  [9.38347002e-05]\n  [2.79861652e-05]\n  [5.36904881e-05]\n  [5.30327816e-05]\n  [4.22764715e-05]\n  [2.72710149e-05]\n  [2.57259289e-05]\n  [1.68999977e-05]\n  [4.35765696e-06]\n  [2.67985706e-06]]\n\n [[2.65325616e-05]\n  [2.22640974e-05]\n  [5.44054456e-06]\n  [3.14451290e-06]\n  [1.02875720e-06]\n  [9.48435456e-07]\n  [2.31034278e-06]\n  [2.19868480e-06]\n  [1.58366729e-05]\n  [6.15024474e-05]\n  [2.55121471e-04]\n  [1.68213267e-02]\n  [7.52670765e-01]\n  [9.79493439e-01]\n  [8.76257002e-01]\n  [4.04806584e-02]\n  [5.64797432e-04]\n  [9.87099920e-05]\n  [3.09095158e-05]\n  [1.04317023e-05]\n  [2.46056061e-05]\n  [3.28375245e-05]\n  [2.11123606e-05]\n  [1.30824301e-05]\n  [1.30931012e-05]\n  [8.63152309e-06]\n  [3.86030752e-06]\n  [3.91848153e-06]]\n\n [[4.68516155e-05]\n  [5.13559935e-05]\n  [1.59425781e-05]\n  [6.20111587e-06]\n  [1.02747572e-06]\n  [6.26572330e-07]\n  [9.96068820e-07]\n  [1.24028963e-06]\n  [1.41254595e-05]\n  [1.43946861e-04]\n  [1.94465311e-03]\n  [1.23784266e-01]\n  [9.32785571e-01]\n  [9.54998970e-01]\n  [3.16765010e-01]\n  [2.97804782e-03]\n  [7.26707440e-05]\n  [2.29988982e-05]\n  [7.98256224e-06]\n  [2.46853938e-06]\n  [6.56186694e-06]\n  [9.11245024e-06]\n  [5.44137993e-06]\n  [4.73895398e-06]\n  [6.22266180e-06]\n  [6.56394513e-06]\n  [4.70479381e-06]\n  [6.74929424e-06]]\n\n [[1.00372112e-04]\n  [1.29294684e-04]\n  [3.86237407e-05]\n  [1.46632601e-05]\n  [2.04039611e-06]\n  [8.48660477e-07]\n  [1.05140634e-06]\n  [1.52551343e-06]\n  [2.85510287e-05]\n  [5.74266771e-04]\n  [1.00318817e-02]\n  [5.53016305e-01]\n  [9.79152143e-01]\n  [8.99857759e-01]\n  [4.75107543e-02]\n  [5.01415401e-04]\n  [2.68629319e-05]\n  [1.11382415e-05]\n  [4.95590530e-06]\n  [1.88341767e-06]\n  [4.81309962e-06]\n  [7.75451099e-06]\n  [6.73445447e-06]\n  [6.83348344e-06]\n  [1.15209687e-05]\n  [1.13723027e-05]\n  [1.03129041e-05]\n  [1.29591872e-05]]\n\n [[3.05746857e-04]\n  [6.83404971e-04]\n  [3.46025598e-04]\n  [1.33497393e-04]\n  [1.69948853e-05]\n  [5.05594562e-06]\n  [3.24810412e-06]\n  [4.20918695e-06]\n  [8.35761821e-05]\n  [1.56474556e-03]\n  [5.47184050e-02]\n  [8.77037346e-01]\n  [9.65660632e-01]\n  [5.08035421e-01]\n  [9.08358302e-03]\n  [2.47888354e-04]\n  [2.51330202e-05]\n  [1.16056490e-05]\n  [4.47221100e-06]\n  [1.44867397e-06]\n  [2.85900592e-06]\n  [5.12927045e-06]\n  [7.16790464e-06]\n  [9.91645629e-06]\n  [1.52065486e-05]\n  [1.49609505e-05]\n  [1.35639921e-05]\n  [1.60244344e-05]]\n\n [[3.98053700e-04]\n  [6.48779329e-04]\n  [3.07304173e-04]\n  [7.78461617e-05]\n  [9.02467855e-06]\n  [3.29992099e-06]\n  [3.60530635e-06]\n  [4.59369994e-06]\n  [5.66091876e-05]\n  [2.37045693e-03]\n  [1.49519518e-01]\n  [8.89193416e-01]\n  [8.53895009e-01]\n  [1.20345898e-01]\n  [1.88301294e-03]\n  [9.86697196e-05]\n  [1.95779812e-05]\n  [6.56875409e-06]\n  [1.66454697e-06]\n  [5.74295029e-07]\n  [9.43219277e-07]\n  [2.39311316e-06]\n  [6.56371367e-06]\n  [1.39609356e-05]\n  [2.41333382e-05]\n  [2.27107848e-05]\n  [2.16987646e-05]\n  [2.68987442e-05]]\n\n [[1.69545849e-04]\n  [2.25794487e-04]\n  [8.14514133e-05]\n  [3.05341855e-05]\n  [6.10527422e-06]\n  [3.30507351e-06]\n  [8.39165114e-06]\n  [1.35332148e-05]\n  [1.54244946e-04]\n  [2.79357028e-03]\n  [3.95039357e-02]\n  [3.10930133e-01]\n  [2.19034418e-01]\n  [1.00446511e-02]\n  [2.55520688e-04]\n  [2.84738617e-05]\n  [5.47118043e-06]\n  [1.24252949e-06]\n  [2.27608396e-07]\n  [1.10479633e-07]\n  [1.90918982e-07]\n  [4.51995845e-07]\n  [1.92190942e-06]\n  [7.59335126e-06]\n  [2.07630601e-05]\n  [2.37955210e-05]\n  [2.75965504e-05]\n  [3.33935750e-05]]\n\n [[1.23701803e-03]\n  [4.70744330e-04]\n  [1.22234254e-04]\n  [6.25019020e-05]\n  [2.25779768e-05]\n  [1.86066572e-05]\n  [3.10772775e-05]\n  [5.01415270e-05]\n  [2.69903423e-04]\n  [1.15884887e-03]\n  [4.51339921e-03]\n  [7.07736006e-03]\n  [3.39135900e-03]\n  [5.72808320e-04]\n  [6.79276054e-05]\n  [8.59484680e-06]\n  [2.61443051e-06]\n  [6.87992383e-07]\n  [1.19137610e-07]\n  [1.00072711e-07]\n  [1.75992554e-07]\n  [3.28847165e-07]\n  [1.01768092e-06]\n  [2.86591535e-06]\n  [8.59650299e-06]\n  [2.37127460e-05]\n  [5.37177839e-05]\n  [2.44816358e-04]]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 15\nplt.figure(figsize=(20, 4))\nfor i in range(1,n):\n    # display original noisy image\n    ax = plt.subplot(2, n, i)\n    plt.imshow(input_x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display denosined image\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}