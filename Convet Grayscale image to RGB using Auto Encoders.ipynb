{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.layers import Conv2D, Flatten\nfrom tensorflow.keras.layers import Reshape, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the CIFAR10 data\n(x_train, _), (x_test, _) = cifar10.load_data()\n\n# input image dimensions\n# we assume data format \"channels_last\"\nimg_rows = x_train.shape[1]\nimg_cols = x_train.shape[2]\nchannels = x_train.shape[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create saved_images folder\nimgs_dir = 'saved_images'\nsave_dir = os.path.join(os.getcwd(), imgs_dir)\nif not os.path.isdir(save_dir):\n        os.makedirs(save_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the 1st 100 input images (color and gray)\nimgs = x_test[:100]\nimgs = imgs.reshape((10, 10, img_rows, img_cols, channels))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nplt.figure()\nplt.axis('off')\nplt.title('Test color images (Ground  Truth)')\nplt.imshow(imgs, interpolation='none')\nplt.savefig('%s/test_color.png' % imgs_dir)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert color train and test images to gray\nx_train_gray = rgb2gray(x_train)\nx_test_gray = rgb2gray(x_test)\n\n# display grayscale version of test images\nimgs = x_test_gray[:100]\nimgs = imgs.reshape((10, 10, img_rows, img_cols))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nplt.figure()\nplt.axis('off')\nplt.title('Test gray images (Input)')\nplt.imshow(imgs, interpolation='none', cmap='gray')\nplt.savefig('%s/test_gray.png' % imgs_dir)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize output train and test color images\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# normalize input train and test grayscale images\nx_train_gray = x_train_gray.astype('float32') / 255\nx_test_gray = x_test_gray.astype('float32') / 255\n\n# reshape images to row x col x channel for CNN output/validation\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n\n# reshape images to row x col x channel for CNN input\nx_train_gray = x_train_gray.reshape(x_train_gray.shape[0], img_rows, img_cols, 1)\nx_test_gray = x_test_gray.reshape(x_test_gray.shape[0], img_rows, img_cols, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# network parameters\ninput_shape = (img_rows, img_cols, 1)\nbatch_size = 32\nkernel_size = 3\nlatent_dim = 256\n# encoder/decoder number of CNN layers and filters per layer\nlayer_filters = [64, 128, 256]\n\n# build the autoencoder model\n# first build the encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\n# stack of Conv2D(64)-Conv2D(128)-Conv2D(256)\nfor filters in layer_filters:\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               strides=2,\n               activation='relu',\n               padding='same')(x)\n\n# shape info needed to build decoder model so we don't do hand computation\n# the input to the decoder's first Conv2DTranspose will have this shape\n# shape is (4, 4, 256) which is processed by the decoder back to (32, 32, 3)\nshape = K.int_shape(x)\n\n# generate a latent vector\nx = Flatten()(x)\nlatent = Dense(latent_dim, name='latent_vector')(x)\n\n# instantiate encoder model\nencoder = Model(inputs, latent, name='encoder')\nencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\nx = Dense(shape[1]*shape[2]*shape[3])(latent_inputs)\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\n# stack of Conv2DTranspose(256)-Conv2DTranspose(128)-Conv2DTranspose(64)\nfor filters in layer_filters[::-1]:\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        strides=2,\n                        activation='relu',\n                        padding='same')(x)\n\noutputs = Conv2DTranspose(filters=channels,\n                          kernel_size=kernel_size,\n                          activation='sigmoid',\n                          padding='same',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# autoencoder = encoder + decoder\n# instantiate autoencoder model\nautoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\nautoencoder.summary()\n\n\n# prepare model saving directory.\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'colorized_ae_model.{epoch:03d}.h5'\nif not os.path.isdir(save_dir):\n        os.makedirs(save_dir)\nfilepath = os.path.join(save_dir, model_name)\n\n# reduce learning rate by sqrt(0.1) if the loss does not improve in 5 epochs\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                               cooldown=0,\n                               patience=5,\n                               verbose=1,\n                               min_lr=0.5e-6)\n\n# save weights for future use (e.g. reload parameters w/o training)\ncheckpoint = ModelCheckpoint(filepath=filepath,\n                             monitor='val_loss',\n                             verbose=1,\n                             save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Square Error (MSE) loss function, Adam optimizer\nautoencoder.compile(loss='mse', optimizer='adam')\n\n# called every epoch\ncallbacks = [lr_reducer, checkpoint]\n\n# train the autoencoder\nautoencoder.fit(x_train_gray,\n                x_train,\n                validation_data=(x_test_gray, x_test),\n                epochs=30,\n                batch_size=batch_size,\n                callbacks=callbacks)\n\n# predict the autoencoder output from test data\nx_decoded = autoencoder.predict(x_test_gray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import array_to_img\n\nplt.imshow(array_to_img(x_test_gray[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(array_to_img(x_decoded[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(array_to_img(x_test[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,3):\n    fig = plt.figure()\n#   fig.suptitle(\"RGB      GroundTruth    Converted\",size=16)\n    ax1 = fig.add_subplot(3,3,1)\n    ax1.set_title(\"Gray Scale\")\n    ax1.imshow(array_to_img(x_test_gray[i]))\n    \n    ax2 = fig.add_subplot(3,3,2)\n    ax2.set_title(\"Ground_Truth\")\n    ax2.imshow(array_to_img(x_test[i]))\n    \n    ax3 = fig.add_subplot(3,3,3)\n    ax3.set_title(\"Converted RGB\")\n    ax3.imshow(array_to_img(x_decoded[i]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}